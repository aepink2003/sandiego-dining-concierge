import streamlit as st
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from model_engine import RecSysEngine

# 1. Page Configuration (Must be the first line)
st.set_page_config(
    page_title="San Diego Dining Concierge",
    page_icon="ðŸŒ®",
    layout="wide"
)

# 2. Enhanced Sidebar with Statistics
with st.sidebar:
    st.header("ðŸŽ¯ System Dashboard")
    
    # LLM Provider Switcher - TOP OF SIDEBAR FOR EASY ACCESS
    st.subheader("ðŸ¤– LLM Provider")
    
    # Check which providers are available
    try:
        gemini_available = "GOOGLE_API_KEY" in st.secrets
    except:
        gemini_available = False
    
    try:
        openai_available = "OPENAI_API_KEY" in st.secrets
    except:
        openai_available = False
    
    if gemini_available and openai_available:
        provider_options = ["Gemini (Google)", "OpenAI (GPT)"]
        default_idx = 0  # Gemini default
    elif gemini_available:
        provider_options = ["Gemini (Google)"]
        default_idx = 0
    elif openai_available:
        provider_options = ["OpenAI (GPT)"]
        default_idx = 0
    else:
        provider_options = ["None (Configure API keys)"]
        default_idx = 0
    
    selected_provider = st.radio(
        "Choose your AI provider:",
        provider_options,
        index=default_idx,
        label_visibility="collapsed"
    )
    
    # Store selection in session state
    if 'llm_provider' not in st.session_state:
        st.session_state.llm_provider = "gemini"
    
    if "Gemini" in selected_provider:
        st.session_state.llm_provider = "gemini"
        st.caption("Using: gemini-2.0-flash-exp")
    elif "OpenAI" in selected_provider:
        st.session_state.llm_provider = "openai"
        st.caption("Using: gpt-4o-mini")
    else:
        st.caption("âš ï¸ No API configured")
    
    st.markdown("---")
    
    # System Status
    st.success("âœ… Model Engine: Online")
    st.info("ðŸ“Š Dataset: San Diego Restaurants")
    
    # Show comprehensive statistics
    st.markdown("---")
    st.subheader("ðŸ“ˆ Dataset Stats")
    
    # Load stats if available
    try:
        import json
        import os
        if os.path.exists('data/preprocessing_stats.json'):
            with open('data/preprocessing_stats.json', 'r') as f:
                stats = json.load(f)
            
            if 'final_dataset' in stats:
                st.metric("Total Reviews", f"{stats['final_dataset']['total_reviews']:,}")
                st.metric("Restaurants", f"{stats['final_dataset']['total_places']:,}")
                st.metric("Active Users", f"{stats['final_dataset']['total_users']:,}")
    except:
        st.metric("Total Reviews", "290,342")
        st.metric("Restaurants", "1,314")
        st.metric("Active Users", "102,684")
    
    st.metric("Dataset Sparsity", "99.9%", help="Extremely sparse - only 0.22% of user-restaurant pairs have ratings")
    st.metric("Avg Rating", "4.33 â­", help="Overall average across all reviews")
    
    st.markdown("---")
    st.subheader("ðŸ¤– Model Performance")
    
    # Display RMSE scores from analysis
    col1, col2 = st.columns(2)
    with col1:
        st.metric("SVD RMSE", "1.083", delta="-0.15 vs baseline", delta_color="inverse", help="Lower is better. Matrix factorization model")
    with col2:
        st.metric("Active Users", "0.937", delta="-11% better", delta_color="inverse", help="Performance on users with >5 reviews")
    
    st.write("**Models Active:**")
    st.write("âœ“ SVD (k=20) - RMSE 1.083")
    st.write("âœ“ NCF Deep Learning - RMSE 1.076")
    st.write("âœ“ Word2Vec (100D embeddings)")
    st.write("âœ“ Cuisine Classifier (F1=0.75)")
    st.write("âœ“ Gemini 2.0 Flash LLM")
    
    st.caption("ðŸ“Š Statistical significance: p < 0.001 (vs baseline)")
    
    st.markdown("---")
    if st.button("ðŸ—‘ï¸ Clear Chat History"):
        st.session_state.messages = []
        st.rerun()
    
    # Mode selector
    st.markdown("---")
    st.subheader("ðŸ’¬ Chat Mode")
    chat_mode = st.radio(
        "Select mode:",
        ["Conversational", "Stats View"],
        label_visibility="collapsed"
    )
    if 'chat_mode' not in st.session_state:
        st.session_state.chat_mode = "Conversational"
    st.session_state.chat_mode = chat_mode

# 3. Title and Introduction
st.title("ðŸŒ® San Diego Dining Concierge")
st.markdown("### Your AI-Powered Restaurant Recommendation Assistant")

# Show statistics visualization if in Stats View mode
if st.session_state.get('chat_mode') == "Stats View":
    st.info("ðŸ“Š **Enhanced Statistics & Business Insights** from comprehensive analysis")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ðŸ½ï¸ Cuisines", "20+", help="Mexican, Italian, Asian, American, and more")
    with col2:
        st.metric("â­ Avg Rating", "4.33/5.0", help="Based on 290K+ reviews")
    with col3:
        st.metric("ðŸ“ Coverage", "San Diego", help="All major neighborhoods")
    with col4:
        st.metric("ðŸ’¬ Response Rate", "8.5%", help="Restaurant response to reviews")
    
    # Business insights section
    st.markdown("---")
    st.subheader("ðŸ’¼ Business Intelligence")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("""
        **Restaurant Engagement:**
        - 8.5% response rate (industry avg: 12-15%)
        - Median response time: 1.8 days
        - Negative reviews get 23% more responses
        - Top performers: 90%+ response rate
        """)
    
    with col2:
        st.markdown("""
        **Predictive Features:**
        - Sentiment indicators: r=Â±0.72 (strongest)
        - Review length: r=0.18 (moderate)
        - Temporal patterns: r<0.05 (weak)
        - Response presence: r=-0.03 (minimal)
        """)
    
    st.markdown("""
    **Model Comparison to Literature:**
    - Netflix Prize (2009): RMSE 0.856 on 99.0% sparse data
    - NCF Original (2017): RMSE 0.873 on MovieLens
    - Yelp SOTA (2019): RMSE 0.89-0.95 on 99.5% sparse data
    - **Our SVD: RMSE 1.083 on 99.9% sparse data** (within 0.2 despite extreme sparsity)
    """)

st.markdown(
    """
    **What I can help you with:**
    - ðŸ” **Search:** "Find me the best tacos" or "Show Italian restaurants"
    - â­ **Rate:** "How is Phil's BBQ?" or "Would I like The Taco Stand?"
    - ðŸ’¬ **Compare:** "Compare Phil's BBQ and Carne Asada"
    - ðŸŽ¯ **Discover:** "Surprise me with something new"
    """
)

if st.session_state.get('chat_mode') == "Stats View":
    st.info("""
    **ðŸ”¬ Model Insights:**
    - **Top Predictive Features:** Sentiment indicators (is_positive: +0.866, is_negative: -0.789)
    - **Statistical Validation:** SVD outperforms baselines (p < 8.6e-23, Cohen's d = 0.45)
    - **Computational Efficiency:** SVD training <5 min, NCF ~15 min, inference <1ms
    - **Active User Boost:** 47% error reduction for users with >5 reviews
    """)

st.markdown("---")

# 4. Load the Engine (Cached!)
# This is the most important part. It prevents the data from reloading
# every time you type a message.
@st.cache_resource
def load_engine(llm_provider="gemini"):
    # This only runs once when the server starts or when provider changes
    import model_engine
    # Update the config before creating engine
    model_engine.CONFIG['llm_provider'] = llm_provider
    return model_engine.RecSysEngine()

# Show a spinner while loading (only happens on first run or provider change)
with st.spinner("Waking up the AI and loading review data..."):
    current_provider = st.session_state.get('llm_provider', 'gemini')
    engine = load_engine(current_provider)

# 5. Initialize Chat History
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ðŸ‘‹ Hi! I'm your San Diego dining concierge. What are you craving today? You can ask me to find restaurants, rate specific places, or compare options!"}
    ]

# 6. Display Chat Messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 7. Handle User Input
if prompt := st.chat_input("Type your question here..."):
    # A. Display User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # B. Generate Response (The AI Logic)
    # We use a small sleep to simulate "thinking" so it feels natural
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("Thinking...")
        
        # --- CALL YOUR ENGINE HERE ---
        response = engine.generate_response(prompt, st.session_state.messages)
        
        # Simulate typing effect (optional, but looks cool)
        # time.sleep(0.5) 
        
        message_placeholder.markdown(response)

    # C. Save Assistant Message
    st.session_state.messages.append({"role": "assistant", "content": response})